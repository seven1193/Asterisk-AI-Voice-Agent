# Asterisk AI Voice Agent - Main Services
#
# DEFAULT NETWORK MODE: Host (for telephony/low-latency)
# - Direct access to host network (127.0.0.1 reaches Asterisk)
# - No port mapping needed
# - Best for telephony integrations
#
# PERMISSION ALIGNMENT:
# If your Asterisk uses a different GID than 995 (FreePBX default):
#   export ASTERISK_GID=$(id -g asterisk)
#   docker compose build ai-engine
#   docker compose up -d
#
# For optional monitoring (Prometheus + Grafana), use:
#   docker compose -f docker-compose.monitoring.yml up -d
#
# See monitoring/README.md for details.

services:
  ai-engine:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        ASTERISK_GID: ${ASTERISK_GID:-995}
    container_name: ai_engine
    # Run as Asterisk user for file permission compatibility (set ASTERISK_UID/GID in .env)
    # Falls back to appuser if not set
    user: "${ASTERISK_UID:-1000}:${ASTERISK_GID:-995}"
    network_mode: host
    volumes:
      - ./src:/app/src
      - ./main.py:/app/main.py
      - ./config:/app/config
      - ./scripts:/app/scripts
      - ./models:/app/models
      - ./asterisk_media:/mnt/asterisk_media
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - HEALTH_BIND_HOST=0.0.0.0
      - ASTERISK_HOST=${ASTERISK_HOST:-127.0.0.1}
    tty: true
    stdin_open: true
    restart: unless-stopped

  local-ai-server:
    build:
      context: ./local_ai_server
      dockerfile: Dockerfile
      args:
        - INCLUDE_KROKO_EMBEDDED=${INCLUDE_KROKO_EMBEDDED:-true}
    container_name: local_ai_server
    network_mode: host
    env_file:
      - .env
    volumes:
      - ./models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - LOCAL_LOG_LEVEL=${LOCAL_LOG_LEVEL:-INFO}
      - LOCAL_DEBUG=${LOCAL_DEBUG:-0}
      # STT Configuration
      - LOCAL_STT_BACKEND=${LOCAL_STT_BACKEND:-vosk}
      - LOCAL_STT_MODEL_PATH=${LOCAL_STT_MODEL_PATH:-/app/models/stt/vosk-model-en-us-0.22}
      - LOCAL_STT_IDLE_MS=${LOCAL_STT_IDLE_MS:-5000}
      # LLM Configuration
      - LOCAL_LLM_MODEL_PATH=${LOCAL_LLM_MODEL_PATH:-/app/models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf}
      - LOCAL_LLM_THREADS=${LOCAL_LLM_THREADS:-16}
      - LOCAL_LLM_CONTEXT=${LOCAL_LLM_CONTEXT:-768}
      - LOCAL_LLM_BATCH=${LOCAL_LLM_BATCH:-128}
      - LOCAL_LLM_MAX_TOKENS=${LOCAL_LLM_MAX_TOKENS:-64}
      - LOCAL_LLM_TEMPERATURE=${LOCAL_LLM_TEMPERATURE:-0.4}
      - LOCAL_LLM_TOP_P=${LOCAL_LLM_TOP_P:-0.85}
      - LOCAL_LLM_REPEAT_PENALTY=${LOCAL_LLM_REPEAT_PENALTY:-1.05}
      - LOCAL_LLM_USE_MLOCK=${LOCAL_LLM_USE_MLOCK:-0}
      - LOCAL_LLM_INFER_TIMEOUT_SEC=${LOCAL_LLM_INFER_TIMEOUT_SEC:-30}
      # GPU: 0=CPU only, -1=auto-detect, N=specific layers
      - LOCAL_LLM_GPU_LAYERS=${LOCAL_LLM_GPU_LAYERS:-0}
      # TTS Configuration
      - LOCAL_TTS_BACKEND=${LOCAL_TTS_BACKEND:-piper}
      - LOCAL_TTS_MODEL_PATH=${LOCAL_TTS_MODEL_PATH:-/app/models/tts/en_US-lessac-medium.onnx}
      - KOKORO_VOICE=${KOKORO_VOICE:-af_heart}
      - LOCAL_LLM_SYSTEM_PROMPT=${LOCAL_LLM_SYSTEM_PROMPT:-You are a helpful AI voice assistant. When the caller wants to end the call or says goodbye, output <tool_call>{"name":"hangup_call","arguments":{"farewell":"Goodbye, have a great day!"}}</tool_call> and say a brief farewell. When the caller asks to email the transcript, output <tool_call>{"name":"request_transcript","arguments":{"email":"caller@example.com"}}</tool_call>. Always provide a spoken response.}
    tty: true
    stdin_open: true
    restart: unless-stopped
    # Uncomment below for NVIDIA GPU support:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import asyncio; import websockets; async def check(): async with websockets.connect('ws://127.0.0.1:8765', ping_interval=None) as ws: await ws.close(); asyncio.run(check())\""]
      interval: 60s
      timeout: 5s
      retries: 180
      start_period: 120s

  admin-ui:
    build:
      context: ./admin_ui
      dockerfile: Dockerfile
    container_name: admin_ui
    network_mode: host
    volumes:
      - ./:/app/project
      - /var/run/docker.sock:/var/run/docker.sock
      - /usr/bin/docker:/usr/bin/docker:ro
      - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose:ro
      - /etc/os-release:/host/etc/os-release:ro
    environment:
      - PROJECT_ROOT=/app/project
      - UVICORN_PORT=3003
    restart: unless-stopped
